{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer report",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUP0X8h8HU/InJHmnKHlS9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICTKtD99rmN3",
        "colab_type": "text"
      },
      "source": [
        "# Data Collection for transfer analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgb105GKrxNJ",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "We will be using bs4 to parse through the html pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF0yHyk9ygrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from urllib.request import Request, urlopen\n",
        "from random import randint,shuffle\n",
        "import time\n",
        "from time import sleep\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OscN1ilzlDLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install user_agent\n",
        "import user_agent\n",
        "from user_agent import generate_user_agent, generate_navigator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAMpL2LxsDZz",
        "colab_type": "text"
      },
      "source": [
        "## Scraping News Websites for traffic news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVpq5q0YsO2w",
        "colab_type": "text"
      },
      "source": [
        "### Skysports\n",
        "Skysports does not hold old transfer news (only 12 pages of it) on their website but they are lenient when it comes to scrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5ee9Nqy3mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content = []\n",
        "skysports = [\n",
        "       \"https://www.skysports.com/football/transfer-news\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/2\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/3\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/4\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/5\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/6\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/7\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/8\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/9\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/10\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/11\",\n",
        "       \"https://www.skysports.com/football/transfer-news/more/12\"\n",
        "       ]\n",
        "for url in skysports:\n",
        "    page = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    regex = re.compile('news-item')\n",
        "    content_lis = soup.find_all('div', attrs={'class': regex})\n",
        "    for div in content_lis:\n",
        "        title = div.find('h2').getText()\n",
        "        link = div.find('a')[\"href\"]\n",
        "        x = re.search(\"^/transfer\" , link)\n",
        "        if x != None :\n",
        "          link = \"https://www.skysports.com\" + link;\n",
        "        topic = div.find('strong').getText().strip()\n",
        "        newspage = urllib.request.urlopen(link)\n",
        "        tsoup = BeautifulSoup(newspage, 'html.parser')\n",
        "        try:\n",
        "          article_title = tsoup.find('h1').getText()\n",
        "        except:\n",
        "          article_title = \"\"\n",
        "        try:\n",
        "          article_subtitle = tsoup.find('h2').getText()\n",
        "        except:\n",
        "          article_subtitle = \"\"\n",
        "        h3_array = tsoup.find_all('h3')\n",
        "        article_part = []\n",
        "        for h3 in h3_array : \n",
        "            article_part.append(h3.getText())\n",
        "        article_part = '\\n'.join(article_part)\n",
        "        p_array = tsoup.find_all('p')\n",
        "        article_content = []\n",
        "        for h3 in p_array : \n",
        "            article_content.append(h3.getText())\n",
        "        article_content = '\\n'.join(article_content)\n",
        "        regex = re.compile(\"highlight\")\n",
        "        try:\n",
        "          date = tsoup.find(\"span\" , attrs={\"class\":regex}).getText()\n",
        "        except:\n",
        "          date = \"\"\n",
        "        article = {\n",
        "            \"url\" : link ,\n",
        "            \"subscript\" : title ,\n",
        "            \"topic\" : topic ,\n",
        "            \"title\" : article_title ,\n",
        "            \"subtitle\" : article_subtitle ,\n",
        "            \"emphasis\" : article_part ,\n",
        "            \"content\" : article_content ,\n",
        "            \"date\" : date\n",
        "        }\n",
        "        content.append(article)\n",
        "df = pd.DataFrame(content)\n",
        "df.to_csv(\"skysport_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S89Smkn_sibj",
        "colab_type": "text"
      },
      "source": [
        "### Goal\n",
        "Goal.com has all its articles accessible but they have a strict robots.txt so we'll have to use a user agent to scrape data\n",
        "We'll also put a time delay as we are scraping a lot of information we do not want overload their servers with requests.\n",
        "\n",
        "* **Irresponsible web scraping can get you blocked from websites** \n",
        "* **Data scraped belongs to goal.com. Any form of redistribution will have legal consequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA3_Dp_j6uNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers = {'User-Agent': generate_user_agent()}\n",
        "content = []\n",
        "links = []\n",
        "goal = [\n",
        "        \"https://www.goal.com/en-us/transfer-news\"\n",
        "       ]\n",
        "for i in range(2,501):\n",
        "  new_url = goal[0]+\"/\"+str(i)\n",
        "  goal.append(new_url)\n",
        "shuffle(goal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwM6COoP7-qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "n = 0\n",
        "program_starts = time.time()\n",
        "for url in goal:\n",
        "  print(url)\n",
        "  now = time.time()\n",
        "  print(\"{0} sites have been scraped .It has been {1} seconds\".format(n,now - program_starts))\n",
        "  sleep(randint(1,30))\n",
        "  request = Request(url, headers=headers)\n",
        "  page = urllib.request.urlopen(request)\n",
        "  soup = BeautifulSoup(page, 'html.parser')\n",
        "  content_lis = soup.find_all('article')\n",
        "  for article in content_lis :\n",
        "    regex = re.compile(\"article\")\n",
        "    link = article.find('a', attrs={'class': regex})\n",
        "    if link != None:\n",
        "      link = \"https://www.goal.com\"+link[\"href\"]  \n",
        "      links.append(link)\n",
        "  n = n + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gR2nrebuRUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "links = pd.DataFrame(links)\n",
        "links.to_csv(\"goal.csv\")\n",
        "links"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlHH0CsqgGtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"skysport_data.csv\")\n",
        "links = pd.read_csv(\"goal.csv\")\n",
        "links = list(links[\"0\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIUjjbElOr-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle(links)\n",
        "links"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ivCRgVbO5LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers = {'User-Agent': generate_user_agent()}\n",
        "n = 0\n",
        "program_starts = time.time()\n",
        "for url in links:\n",
        "  try:\n",
        "    n = n+1\n",
        "    request = Request(url, headers=headers)\n",
        "    page = urllib.request.urlopen(request)\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    sleep(randint(1,10))\n",
        "    try:\n",
        "      article_title = soup.find(\"h1\").getText()\n",
        "    except:\n",
        "      article_title = \"\"\n",
        "    try:\n",
        "      regex = re.compile(\"teaser\")\n",
        "      article_subtitle = soup.find(\"div\",attrs={\"class\" : regex}).getText()\n",
        "    except:\n",
        "      article_subtitle = \"\"\n",
        "    h3_array = soup.find_all('h2')\n",
        "    article_part = []\n",
        "    for h3 in h3_array : \n",
        "        article_part.append(h3.getText())\n",
        "    article_part = '\\n'.join(article_part)\n",
        "    p_array = soup.find_all('p')\n",
        "    article_content = []\n",
        "    for h3 in p_array : \n",
        "        article_content.append(h3.getText())\n",
        "    article_content = '\\n'.join(article_content)\n",
        "    regex = re.compile(\"tags-list__link\")\n",
        "    taglist = soup.find_all(\"a\",attrs={\"class\" : regex})\n",
        "    tags = []\n",
        "    for tag in taglist:\n",
        "      tags.append(tag.getText())\n",
        "    topic = '+'.join(tags)\n",
        "    datetag = soup.find(\"time\")\n",
        "    date = datetag.getText()\n",
        "    article = {\n",
        "              \"url\" : url ,\n",
        "              \"subscript\" : article_title ,\n",
        "              \"topic\" : topic ,\n",
        "              \"title\" : article_title ,\n",
        "              \"subtitle\" : article_subtitle ,\n",
        "              \"emphasis\" : article_part ,\n",
        "              \"content\" : article_content ,\n",
        "              \"date\" : date.strip()\n",
        "          }\n",
        "    df = df.append(article , ignore_index=True)  \n",
        "    df.to_csv(\"transfer_data.csv\")\n",
        "    now = time.time()\n",
        "    print(\"{0} sites have been scraped .It has been {1} seconds\".format(n,now - program_starts))\n",
        "  except:\n",
        "    n=n-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdXZFGu1aX-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"final.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDc0Ncosbm-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ4wi7KqcRA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}